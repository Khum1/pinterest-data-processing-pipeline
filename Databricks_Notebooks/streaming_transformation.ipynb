{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a336a6ed-a909-48bc-8b2b-96adcf246f07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Streaming Transformation using Databricks and Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aa93cd6-875d-4757-8a32-2f4ec9687b07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[1]: [FileInfo(path=&#39;dbfs:/FileStore/tables/authentication_credentials.csv&#39;, name=&#39;authentication_credentials.csv&#39;, size=202, modificationTime=1674066475000)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[1]: [FileInfo(path=&#39;dbfs:/FileStore/tables/authentication_credentials.csv&#39;, name=&#39;authentication_credentials.csv&#39;, size=202, modificationTime=1674066475000)]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9907618b-51e6-44da-a2ea-4e277c3948de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### Reads data from Kinesis Streams into the Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "685d8a76-44dc-427d-972d-da3a45fdb240",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "file_type = \"csv\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86e77f96-25fe-436d-b9f4-e777da5efad8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Functions created to mitigate repeat code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7acaca1d-8648-4e27-88c0-03879eeb8b8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True) #Deletes the checkpoint folder so that the write command can be run again\n",
    "\n",
    "def create_dataframe_from_stream_data(type_of_record):\n",
    "    '''\n",
    "    Creates a dataframe from the incoming streaming data and produces a dataframe in a json string format\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    type_of_record: str\n",
    "        the type of record that is being added to the df e.g. \"pin\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df\n",
    "        the dataframe that has been created\n",
    "    '''\n",
    "    df = spark.readStream \\\n",
    "        .format(\"kinesis\") \\\n",
    "        .option(\"streamName\", f\"streaming-0a4e65e909bd-{type_of_record}\") \\\n",
    "        .option(\"region\", \"us-east-1\") \\\n",
    "        .option(\"initialPosition\", 'earliest') \\\n",
    "        .option(\"awsAccessKey\", ACCESS_KEY) \\\n",
    "        .option(\"awsSecretKey\", SECRET_KEY) \\\n",
    "        .load()\n",
    "    df = df.selectExpr(\"CAST(data as STRING)\")\n",
    "    return df\n",
    "\n",
    "def normalise_follower_count():\n",
    "    '''\n",
    "    Changes the follower count from a string, to an integer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    df = pin_df.withColumn(\"follower_count\", when(\n",
    "        col(\"follower_count\").rlike(\"\\\\d+k\"), #Checks if the value matches a pattern of one or more digits plus the letter k (e.g. 12k)\n",
    "        (regexp_extract(col(\"follower_count\"), \"(\\\\d+)\", 1).cast(\"integer\") * 1000) \n",
    "    ).otherwise(col(\"follower_count\").cast(\"integer\"))) \n",
    "    return df\n",
    "\n",
    "def create_delta_table(df, type_of_record):\n",
    "    '''\n",
    "    Creates a delta table\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : \n",
    "        the datarfame to be made into a delta table\n",
    "    type_of_record : str\n",
    "        the type of record that is being added to the df e.g. \"pin\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    '''\n",
    "    df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "    .table(f\"0a4e65e909bd_{type_of_record}_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "874dd1a4-a851-4caf-966e-4c94bf418f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Pin dataframe and Delta table created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63d0ad7-374c-40d8-b580-823d04ce6b95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema = StructType([ #Gives the structure of the df for the table to be laid out \n",
    "    StructField(\"index\",StringType(),True), \n",
    "    StructField(\"unique_id\",StringType(),True), \n",
    "    StructField(\"title\",StringType(),True), \n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"downloaded\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "pin_df = create_dataframe_from_stream_data(\"pin\")\n",
    "\n",
    "#Assembles data into df with separate columns\n",
    "pin_df = pin_df.withColumn(\"jsonData\",from_json(col(\"data\"),schema)) \\\n",
    "                   .select(\"jsonData.*\")\n",
    "\n",
    "pin_df = normalise_follower_count()\n",
    "pin_df = pin_df.withColumn(\"save_location\", regexp_extract(col(\"save_location\"), \"(/data/).*\", 0)) #Extracts the save location of the column \n",
    "pin_df = pin_df.withColumnRenamed(\"index\", \"ind\") #Renames the index column to ind to match the geo and user dfs\n",
    "# pin_df = pin_df.withColumn(\"ind\",col(\"ind\").cast(\"integer\")) #Ensures ind column is an integer\n",
    "\n",
    "column_structure = [\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"]\n",
    "pin_df = pin_df.select(column_structure) #Rstructures the column to the order in column_structure\n",
    "\n",
    "create_delta_table(pin_df, \"pin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63c23fd0-97bb-46e3-910b-497e227dc3f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Geo dataframe and Delta table created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1389e7f7-73f3-4bde-905d-2fdad15a92d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "schema = StructType([ \n",
    "    StructField(\"ind\",StringType(),True), \n",
    "    StructField(\"latitude\",StringType(),True), \n",
    "    StructField(\"longitude\",StringType(),True), \n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "geo_df = create_dataframe_from_stream_data(\"geo\")\n",
    "geo_df = geo_df.withColumn(\"jsonData\",from_json(col(\"data\"),schema)) \\\n",
    "                   .select(\"jsonData.*\")\n",
    "\n",
    "geo_df = geo_df.withColumn(\"coordinates\", array(col(\"longitude\"), col(\"latitude\")))\n",
    "geo_df = geo_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "column_structure = [\"ind\", \"country\", \"coordinates\", \"timestamp\"]\n",
    "geo_df = geo_df.select(column_structure)\n",
    "\n",
    "create_delta_table(geo_df, \"geo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4af38c9-8c95-4bea-b611-8e429bfbbd94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### User dataframe and Delta table created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289ed220-add7-4334-876a-27e2844d897b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema = StructType([ \n",
    "    StructField(\"ind\",StringType(),True), \n",
    "    StructField(\"first_name\",StringType(),True), \n",
    "    StructField(\"last_name\",StringType(),True), \n",
    "    StructField(\"age\", StringType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True)\n",
    "  ])\n",
    "\n",
    "user_df = create_dataframe_from_stream_data(\"user\")\n",
    "user_df = user_df.withColumn(\"jsonData\",from_json(col(\"data\"),schema)) \\\n",
    "                   .select(\"jsonData.*\")\n",
    "\n",
    "user_df = user_df.withColumn(\"user_name\", concat(col(\"first_name\"),col(\"last_name\")))\n",
    "user_df = user_df.drop(\"first_name\", \"last_name\")\n",
    "user_df = user_df.withColumn(\"date_joined\", col(\"date_joined\").cast(\"timestamp\"))\n",
    "\n",
    "column_structure = [\"ind\", \"user_name\", \"age\", \"date_joined\"]\n",
    "user_df = user_df.select(column_structure)\n",
    "\n",
    "create_delta_table(user_df, \"user\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "streaming_transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
